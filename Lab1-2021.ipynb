{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4035 - Cyber Data Analytics\n",
    "## Lab 1 - Fraud data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit on brightspace (zip file with the name Group_xx.zip)\n",
    "(i) This jupyter file completed with code, plots, figures and report for each question. Additional plots and figures can be created for explanation before the end of each main question. Lab 1 contains 5 main questions, including the bonus. Write the code or explanation below each sub question. For the explanations, include what you would normally include in the report for this lab assignment, for example data pre-processing, hypothesis tested, approach, results, etc.\n",
    "(ii) The libraries needed to run this file. Except for numpy, scikit-learn, pandas, matplotlib\n",
    "\n",
    "Your peers should be able to use the readme section for instructions and be able to run this file. \n",
    "\n",
    "Make sure to keep your answers concise. Maximum number of words is 1000, which you can count with the code below. (You can add around 600 words since we start at around 400)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 369\n"
     ]
    }
   ],
   "source": [
    "# If this cell does not work try running `pip install nbformat`\n",
    "\n",
    "import io\n",
    "import nbformat\n",
    "\n",
    "nb = nbformat.read(\"Lab1-2021.ipynb\", nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "\n",
    "print(\"Word count:\", word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Number :31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 1 \n",
    "### Name :Yuqian Song\n",
    "### ID :4630521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 2\n",
    "### Name :\n",
    "### ID :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readme - Provide instructions - libraries used, location of the data file, etc. Keep it short. Remember your peers will not debug your code and should be able to reproduce the exact output you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reqs: numpy, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualization task – 1 A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Plot visulations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data = pd.read_csv('train_data.csv', encoding='utf-8-sig', sep='\\s*,\\s*', engine='python')\n",
    "sorted_data = data.sort_values(by='amount', ascending=True)\n",
    "p1=sns.kdeplot(sorted_data['amount'], fill=True, color=\"r\")\n",
    "df=data.groupby(['txvariantcode','label']).size()\n",
    "df=df.unstack()\n",
    "df.plot(kind='bar')\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Describe the visualizations and provide relavant explanations of features and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imbalance task – 1 A4 – Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Print ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RocCurveDisplay\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Data processing\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "#Data processing\n",
    "\n",
    "data = data.to_numpy()\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1].astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#Apply SMOTE\n",
    "sm = SMOTE(random_state=20)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "#SVM\n",
    "Classifier = SVC(kernel=\"linear\")\n",
    "Classifier.fit(X_res, np.ravel(y_res, order='C'))\n",
    "y_pred = Classifier.predict(X_test)\n",
    "y_proba = SVC.predict_proba(X_test)\n",
    "\n",
    "#RF\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_res, np.ravel(y_res, order='C'))\n",
    "y_predrfc = rfc.predict(X_test)\n",
    "y_probafc = rfc.predict_proba(X_test)\n",
    "\n",
    "#LR\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_res, np.ravel(y_res, order='C'))\n",
    "y_predlr = lr.predict(X_test)\n",
    "y_probalr = lr.predict_proba(X_test)\n",
    "\n",
    "def plot_sklearn_roc_curve(y_real, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "    roc_display.figure_.set_size_inches(5,5)\n",
    "    plt.plot([0, 1], [0, 1], color = 'g')\n",
    "    \n",
    "plot_sklearn_roc_curve(y_test, y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Analyse the performance of the classifiers. Explain which method performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Is using SMOTE a good idea? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Privacy task – 1 A4 – Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Print ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Analyse the performance of the classifiers. Explain which method performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Can you explain the performance difference for the different classifiers? Is it advisable to protect people’s privacy using rank-swapping? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification task – 2 A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Print relevant plots and metrics with clear headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Explain the applied data pre-processing steps, learning algorithms, and post-processing steps or ensemble methods. Compare the performance of the two algorithms, focusing on performance criteria that are relevant in practice, use 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus task – 1 A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Provide code and report below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
